{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a94fc11",
   "metadata": {},
   "source": [
    "# 01_scrape_uk_housing.ipynb\n",
    "\n",
    "**Authors:** Natan Wojtowicz.\n",
    "\n",
    "**Purpose / short description (explainable in oral exam):**\n",
    "This notebook contains reproducible steps to obtain the HM Land Registry \"Price Paid\" dataset from Kaggle (or to load a local copy if you've uploaded it to the project). It handles the fact that the dataset is large (~2GB) and provides options to:\n",
    "\n",
    "* download via the Kaggle API\n",
    "* verify checksum and file size\n",
    "* extract and inspect the CSV header\n",
    "* create smaller reproducible subsets (time-based, location-based, stratified sampling by county & year)\n",
    "* save the resulting subset(s) to Parquet/CSV for faster downstream processing\n",
    "\n",
    "All code cells contain comments and a short markdown explanation above them so everyone in the group can explain what each cell does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa5632e",
   "metadata": {},
   "source": [
    "## Notebook content\n",
    "\n",
    "### 1. Setup (packages, env notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a38d981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: c:\\Users\\natan\\Desktop\\School\\3eJaar\\ML\\Challenge\\CloudAikes\\dataset_1_uk_housing\\data\\uk_housing\n"
     ]
    }
   ],
   "source": [
    "# Who worked on this cell: Natan\n",
    "# Purpose: install & import packages, notes about Kaggle credentials\n",
    "\n",
    "# If running in a clean environment, install required packages\n",
    "# !pip install kaggle pandas pyarrow dask[complete] tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import json\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Set paths\n",
    "ROOT = Path.cwd()\n",
    "DATA_DIR = ROOT / \"data\" / \"uk_housing\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "RAW_DIR.mkdir(exist_ok=True)\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "PROCESSED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Data directory:\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900bd3ad",
   "metadata": {},
   "source": [
    "**Notes to exam answer:** We use pandas for small tests and PyArrow/parquet for faster disk I/O on large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde1687e",
   "metadata": {},
   "source": [
    "### 2. Downloading from Kaggle (automated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01822c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you cannot run the Kaggle API here, upload the dataset's CSV to data/uk_housing/raw/\n"
     ]
    }
   ],
   "source": [
    "# Who: Natan\n",
    "# Purpose: download the Kaggle dataset programmatically if Kaggle credentials are provided\n",
    "\n",
    "# Kaggle dataset: hm-land-registry/uk-housing-prices-paid\n",
    "# Precondition: user has a kaggle.json file with API credentials in ~/.kaggle/kaggle.json\n",
    "# If running in Binder / Colab you must upload kaggle.json or set env variables.\n",
    "\n",
    "KAGGLE_DATASET = \"hm-land-registry/uk-housing-prices-paid\"\n",
    "TARGET_ZIP = RAW_DIR / \"uk_housing_price_paid.zip\"\n",
    "\n",
    "def kaggle_download(dataset, dest_folder):\n",
    "    # Try to import kaggle CLI programmatically\n",
    "    try:\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"kaggle package not available. Install with `pip install kaggle` and ensure kaggle.json is present.\")\n",
    "\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    print(\"Authenticated to Kaggle\")\n",
    "    # download by dataset\n",
    "    api.dataset_download_files(dataset, path=str(dest_folder), unzip=True, quiet=False)\n",
    "    print(\"Download complete\")\n",
    "\n",
    "# Uncomment to run\n",
    "# kaggle_download(KAGGLE_DATASET, RAW_DIR)\n",
    "\n",
    "print(\"If you cannot run the Kaggle API here, upload the dataset's CSV to data/uk_housing/raw/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c5b96",
   "metadata": {},
   "source": [
    "**Notes:** Many students cannot use Kaggle API due to credentials; we provide manual upload fallback. The dataset on Kaggle typically extracts to a CSV or folder; check `RAW_DIR` afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ebab9e",
   "metadata": {},
   "source": [
    "### 3. Inspect the raw CSV header and example rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2651c75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: c:\\Users\\natan\\Desktop\\School\\3eJaar\\ML\\Challenge\\CloudAikes\\dataset_1_uk_housing\\data\\uk_housing\\raw\\price_paid_records.csv\n",
      "Transaction unique identifier,Price,Date of Transfer,Property Type,Old/New,Duration,Town/City,District,County,PPDCategory Type,Record Status - monthly file only\n",
      "{81B82214-7FBC-4129-9F6B-4956B4A663AD},25000,1995-08-18 00:00,T,N,F,OLDHAM,OLDHAM,GREATER MANCHESTER,A,A\n",
      "{8046EC72-1466-42D6-A753-4956BF7CD8A2},42500,1995-08-09 00:00,S,N,F,GRAYS,THURROCK,THURROCK,A,A\n",
      "{278D581A-5BF3-4FCE-AF62-4956D87691E6},45000,1995-06-30 00:00,T,N,F,HIGHBRIDGE,SEDGEMOOR,SOMERSET,A,A\n",
      "{1D861C06-A416-4865-973C-4956DB12CD12},43150,1995-11-24 00:00,T,N,F,BEDFORD,NORTH BEDFORDSHIRE,BEDFORDSHIRE,A,A\n"
     ]
    }
   ],
   "source": [
    "# Who: Natan\n",
    "# Purpose: quickly inspect the header and first few lines without loading everything into memory\n",
    "\n",
    "CSV_GUESS = RAW_DIR / \"price_paid_records.csv\"  # adjust to actual filename after download\n",
    "if not CSV_GUESS.exists():\n",
    "    # look for any .csv in raw\n",
    "    csvs = list(RAW_DIR.glob('*.csv'))\n",
    "    if csvs:\n",
    "        CSV_GUESS = csvs[0]\n",
    "    else:\n",
    "        print(\"No CSV found in raw folder. Please download or upload the data.\")\n",
    "\n",
    "print(\"Using:\", CSV_GUESS)\n",
    "\n",
    "# Print header + first 3 lines\n",
    "with open(CSV_GUESS, 'r', encoding='utf-8', errors='replace') as f:\n",
    "    for _ in range(5):\n",
    "        print(f.readline().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35c674",
   "metadata": {},
   "source": [
    "**Explain to the examiner:** This cell avoids reading 2GB into memory â€” it just checks the schema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4716dc0b",
   "metadata": {},
   "source": [
    "### 4. Create reproducible subsets (time-based and stratified)\n",
    "\n",
    "We provide two approaches: (A) simple year-based subset (e.g., 2010-2017) and (B) stratified sampling by `county` + `year` to keep distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a8266c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 6200823 rows to c:\\Users\\natan\\Desktop\\School\\3eJaar\\ML\\Challenge\\CloudAikes\\dataset_1_uk_housing\\data\\uk_housing\\processed\\samples\\pp_2010_2017.csv\n"
     ]
    }
   ],
   "source": [
    "# Who: Natan\n",
    "# Purpose: create smaller files to speed up iterative analysis and modeling\n",
    "\n",
    "SAMPLE_DIR = PROCESSED_DIR / \"samples\"\n",
    "SAMPLE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# PARAMETERS\n",
    "start_year = 2010\n",
    "end_year = 2017\n",
    "max_rows_target = 1_000_000  # choose a limit you can work with locally\n",
    "\n",
    "# Approach A: filter by year using chunked read\n",
    "\n",
    "def filter_by_year(input_csv, output_csv, year_from, year_to):\n",
    "    chunks = pd.read_csv(input_csv, parse_dates=[\"Date of Transfer\"], dayfirst=False,\n",
    "                         chunksize=200_000, low_memory=False)\n",
    "    written = False\n",
    "    total = 0\n",
    "    for c in chunks:\n",
    "        c.columns = [col.strip() for col in c.columns]\n",
    "        c['date'] = pd.to_datetime(c['Date of Transfer'], errors='coerce')\n",
    "        c['year'] = c['date'].dt.year\n",
    "        sel = c[(c['year'] >= year_from) & (c['year'] <= year_to)]\n",
    "        if sel.empty:\n",
    "            continue\n",
    "        if not written:\n",
    "            sel.to_csv(output_csv, index=False, mode='w')\n",
    "            written = True\n",
    "        else:\n",
    "            sel.to_csv(output_csv, index=False, header=False, mode='a')\n",
    "        total += len(sel)\n",
    "    print(f\"Wrote {total} rows to {output_csv}\")\n",
    "\n",
    "# Example usage (uncomment to run)\n",
    "filter_by_year(CSV_GUESS, SAMPLE_DIR / 'pp_2010_2017.csv', start_year, end_year)\n",
    "\n",
    "# Approach B: stratified sampling by county+year (chunked)\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def stratified_sample(input_csv, output_csv, n_samples=200_000, stratify_cols=['County', 'year']):\n",
    "    # first pass: compute weights (counts per strata)\n",
    "    counts = defaultdict(int)\n",
    "    reader = pd.read_csv(input_csv, parse_dates=['Date of Transfer'], chunksize=200_000, low_memory=False)\n",
    "    for chunk in tqdm(reader, desc='Counting strata'):\n",
    "        chunk.columns = [c.strip() for c in chunk.columns]\n",
    "        chunk['year'] = pd.to_datetime(chunk['Date of Transfer'], errors='coerce').dt.year\n",
    "        key_series = chunk[stratify_cols].fillna('NA').astype(str).agg('||'.join, axis=1)\n",
    "        for key in key_series:\n",
    "            counts[key] += 1\n",
    "\n",
    "    # compute sampling fractions per stratum proportional to counts\n",
    "    total = sum(counts.values())\n",
    "    fractions = {k: max(1, int(v / total * n_samples)) for k, v in counts.items()}\n",
    "\n",
    "    # second pass: sample rows matching each stratum up to target\n",
    "    samples_written = 0\n",
    "    written_header = False\n",
    "    reader = pd.read_csv(input_csv, parse_dates=['Date of Transfer'], chunksize=200_000, low_memory=False)\n",
    "    sampled_per_key = defaultdict(int)\n",
    "\n",
    "    for chunk in tqdm(reader, desc='Sampling strata'):\n",
    "        chunk.columns = [c.strip() for c in chunk.columns]\n",
    "        chunk['year'] = pd.to_datetime(chunk['Date of Transfer'], errors='coerce').dt.year\n",
    "        chunk['key'] = chunk[stratify_cols].fillna('NA').astype(str).agg('||'.join, axis=1)\n",
    "        to_take = []\n",
    "        for key, group in chunk.groupby('key'):\n",
    "            remain = fractions.get(key, 0) - sampled_per_key.get(key, 0)\n",
    "            if remain <= 0:\n",
    "                continue\n",
    "            # sample min(remain, len(group)) rows\n",
    "            k = min(remain, len(group))\n",
    "            sampled = group.sample(n=k, replace=False, random_state=42)\n",
    "            to_take.append(sampled)\n",
    "            sampled_per_key[key] += len(sampled)\n",
    "\n",
    "        if to_take:\n",
    "            out = pd.concat(to_take)\n",
    "            if not written_header:\n",
    "                out.to_csv(output_csv, index=False, mode='w')\n",
    "                written_header = True\n",
    "            else:\n",
    "                out.to_csv(output_csv, index=False, header=False, mode='a')\n",
    "            samples_written += len(out)\n",
    "            if samples_written >= n_samples:\n",
    "                break\n",
    "\n",
    "    print(f\"Wrote {samples_written} sampled rows to {output_csv}\")\n",
    "\n",
    "# Example usage (uncomment to run)\n",
    "#stratified_sample(CSV_GUESS, SAMPLE_DIR / 'pp_stratified_200k.csv', n_samples=200_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99689862",
   "metadata": {},
   "source": [
    "**Explainable point for oral exam:** The stratified sampling keeps the distribution across counties and years similar to the original, which helps prevent models trained on the sample from learning biased distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3591685b",
   "metadata": {},
   "source": [
    "### 5. Save to Parquet for faster reads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29565d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved c:\\Users\\natan\\Desktop\\School\\3eJaar\\ML\\Challenge\\CloudAikes\\dataset_1_uk_housing\\data\\uk_housing\\processed\\samples\\pp_2010_2017.parquet\n"
     ]
    }
   ],
   "source": [
    "# Who: Natan\n",
    "# Purpose: convert the sample CSV to parquet for faster downstream processing\n",
    "\n",
    "def csv_to_parquet(csv_path, parquet_path):\n",
    "    df = pd.read_csv(csv_path, parse_dates=['Date of Transfer'])\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df.to_parquet(parquet_path, index=False, engine='fastparquet')\n",
    "    print('Saved', parquet_path)\n",
    "\n",
    "# Example usage\n",
    "csv_to_parquet(SAMPLE_DIR / 'pp_2010_2017.csv', SAMPLE_DIR / 'pp_2010_2017.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd38a90",
   "metadata": {},
   "source": [
    "### 6. Notes / next steps\n",
    "\n",
    "* After this notebook you should have `data/uk_housing/processed/samples/*` containing manageable datasets. Use the `02_clean_uk_housing.ipynb` on those files.\n",
    "* If you plan to use the *full* dataset for final training, prefer to convert the CSV to Parquet and use Dask or PySpark to scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
